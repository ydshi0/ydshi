Binary files v1/__pycache__/__init__.cpython-312.pyc and /workspace/vllm/vllm/v1/__pycache__/__init__.cpython-312.pyc differ
Binary files v1/__pycache__/kv_cache_interface.cpython-312.pyc and /workspace/vllm/vllm/v1/__pycache__/kv_cache_interface.cpython-312.pyc differ
Binary files v1/__pycache__/outputs.cpython-312.pyc and /workspace/vllm/vllm/v1/__pycache__/outputs.cpython-312.pyc differ
Binary files v1/__pycache__/request.cpython-312.pyc and /workspace/vllm/vllm/v1/__pycache__/request.cpython-312.pyc differ
Binary files v1/__pycache__/serial_utils.cpython-312.pyc and /workspace/vllm/vllm/v1/__pycache__/serial_utils.cpython-312.pyc differ
Binary files v1/__pycache__/utils.cpython-312.pyc and /workspace/vllm/vllm/v1/__pycache__/utils.cpython-312.pyc differ
Binary files v1/attention/__pycache__/__init__.cpython-312.pyc and /workspace/vllm/vllm/v1/attention/__pycache__/__init__.cpython-312.pyc differ
Binary files v1/attention/backends/__pycache__/__init__.cpython-312.pyc and /workspace/vllm/vllm/v1/attention/backends/__pycache__/__init__.cpython-312.pyc differ
Binary files v1/attention/backends/__pycache__/flash_attn.cpython-312.pyc and /workspace/vllm/vllm/v1/attention/backends/__pycache__/flash_attn.cpython-312.pyc differ
Binary files v1/core/__pycache__/__init__.cpython-312.pyc and /workspace/vllm/vllm/v1/core/__pycache__/__init__.cpython-312.pyc differ
Binary files v1/core/__pycache__/encoder_cache_manager.cpython-312.pyc and /workspace/vllm/vllm/v1/core/__pycache__/encoder_cache_manager.cpython-312.pyc differ
Binary files v1/core/__pycache__/kv_cache_manager.cpython-312.pyc and /workspace/vllm/vllm/v1/core/__pycache__/kv_cache_manager.cpython-312.pyc differ
Binary files v1/core/__pycache__/kv_cache_utils.cpython-312.pyc and /workspace/vllm/vllm/v1/core/__pycache__/kv_cache_utils.cpython-312.pyc differ
Binary files v1/core/__pycache__/scheduler.cpython-312.pyc and /workspace/vllm/vllm/v1/core/__pycache__/scheduler.cpython-312.pyc differ
diff -r v1/core/scheduler.py /workspace/vllm/vllm/v1/core/scheduler.py
8c8
< from vllm.config import CacheConfig, LoRAConfig, ModelConfig, SchedulerConfig
---
> from vllm.config import CacheConfig, LoRAConfig, ModelConfig, SchedulerConfig, KVTransferConfig
18a19,20
> from vllm.distributed import get_kv_transfer_group, has_kv_transfer_group
> 
33a36
>         kv_transfer_config: KVTransferConfig,
40c43
< 
---
>         self.kv_transfer_config = kv_transfer_config
62a66,67
>         self.paused: List[Request] = []
>         self.freeze_id: Set[str] = set()
109c114
< 
---
>         kv_recv_reqs: List[Request] = []
116a122,132
>         # Resume requests paused by kv recv
>         if len(self.paused) > 0:
>             logger.debug(f"before resuming requests: running length {len(self.running)}, paused length {len(self.paused)} ")
>             for request in self.paused:
>                 num_new_tokens = request.num_tokens - request.num_computed_tokens
>                 num_scheduled_tokens[request.request_id] = num_new_tokens
>                 token_budget -= num_new_tokens
>             self.running.extend(self.paused)
>             #self.running = self.paused
>             self.paused.clear()
>             logger.debug(f"after resuming requests: running length {len(self.running)}, paused length {len(self.paused)} ")
118a135
>         logger.debug(f"schedule self.running length {len(self.running)} ")
152a170
>                     logger.warn(f"preempt request {preempted_req.request_id}")
182,183c200
< 
<         # Next, schedule the WAITING requests.
---
>         # Try to schedule kv_recv requests (for disaggregated prefill).
184a202,225
>             token_budget, kv_recv_reqs, recv_req_schedule_tokens = \
>                 self._schedule_kv_recv(token_budget, req_to_new_block_ids)
>             if kv_recv_reqs and len(kv_recv_reqs) > 0:
>                 logger.debug(f"Schedule.schule: schedule kv_recv_reqs {kv_recv_reqs}")
>                 scheduled_new_reqs = kv_recv_reqs
>                 scheduled_running_reqs = list()
>                 scheduled_resumed_reqs = list()
>                 for req in kv_recv_reqs:
>                     self.freeze_id.add(req.request_id)
> 
>             if recv_req_schedule_tokens:
>                 for req_id, num_new_tokens in recv_req_schedule_tokens.items():
>                     num_scheduled_tokens[req_id] = num_new_tokens
>                     # token_budget -= num_new_tokens
> 
>             for request in self.paused:
>                 num_scheduled_tokens.pop(request.request_id)
>                 # token_budget += (request.num_tokens - request.num_computed_tokens)
>                 
>                     
>         reqs_wait_for_kv = []
>         
>         # Next, schedule the WAITING requests.
>         if not preempted_reqs and not kv_recv_reqs:
189a231,236
>                 if request.status == RequestStatus.WAITING and\
>                     self.kv_transfer_config is not None and\
>                     self.kv_transfer_config.is_kv_consumer:
>                     self.waiting.popleft()
>                     reqs_wait_for_kv.append(request)
>                     continue
222a270,275
> 
>                 logger.debug(f"Scheduler.schedule request.num_tokens {request.num_tokens}")
>                 logger.debug(f"Scheduler.schedule computed_blocks {computed_blocks}")
>                 logger.debug(f"Scheduler.schedule num_new_tokens {num_new_tokens}")
>                 logger.debug(f"Scheduler.schedule request {request.request_id}")
>                 logger.debug(f"Scheduler.schedule new_blocks {new_blocks}")
253c306,307
< 
---
>         for request in reqs_wait_for_kv:
>             self.waiting.append(request)
256,257c310,314
<         assert total_num_scheduled_tokens <= self.max_num_scheduled_tokens
<         assert token_budget >= 0
---
>         # ignore token budget for kv_recv requests
>         assert total_num_scheduled_tokens <= self.max_num_scheduled_tokens or\
>             len(kv_recv_reqs) > 0
>         assert token_budget >= 0 or\
>             len(kv_recv_reqs) > 0
296a354
>         logger.debug(f"end of scheduler: total_num_scheduled_tokens {total_num_scheduled_tokens}")
309a368
>             freeze_req_ids=self.freeze_id,
314a374,498
>     def _schedule_kv_recv(
>         self,
>         token_budget: int,
>         req_to_new_block_ids: Dict[str, List[int]],
>     ) -> Tuple[List[Request], Dict[str, int]]:
>         logger.debug("_schedule_kv_recv")
>         logger.debug(f"_schedule_kv_recv self.kv_transfer_config {self.kv_transfer_config}")
> 
>         if not self.kv_transfer_config or not self.kv_transfer_config.is_kv_consumer:
>             return token_budget, None, None
>         logger.debug("_schedule_kv_recv start")
>         debug_waiting = list()
>         for request in self.waiting:
>             debug_waiting.append(request)
>         logger.debug(f"_schedule_kv_recv self.waiting {debug_waiting} len {len(self.waiting)}")
> 
>         prefill_requests = list()
>         remaining_requests = deque()
>         prefill_scheduled_tokens: Dict[str, int] = {}
>         logger.debug(f"_schedule_kv_recv start, the length of waiting list: {len(self.waiting)}")
>         
>         kv_agent = get_kv_transfer_group()
>         for request in self.waiting:
>             computed_blocks, num_computed_tokens = \
>                 self.kv_cache_manager.get_computed_blocks(request)
>             logger.debug(f"scheduler checker request {request.request_id}, prompt len: {request.num_prompt_tokens}, num token {request.num_tokens}, num_computed_tokens {num_computed_tokens}")
>             if len(prefill_requests) > 0:
>                 remaining_requests.append(request)
>                 continue
>             if len(self.running) + len(prefill_requests) == \
>                 self.max_num_running_reqs:
>                 remaining_requests.append(request)
>                 continue
>             # Get already-cached tokens.
>             computed_blocks, num_computed_tokens = \
>                 self.kv_cache_manager.get_computed_blocks(request)
>             logger.debug(f"scheduler checker request num_computed_tokens {num_computed_tokens}")
>             if num_computed_tokens > 0:
>                 remaining_requests.append(request)
>                 logger.debug(f"scheduler checker request {request.request_id} is put into remaining_requests num_computed_tokens >= 0")
>                 continue
>             if not kv_agent.is_kv_ready(request.request_id):
>                 remaining_requests.append(request)
>                 logger.info(f"scheduler request {request.request_id} is not ready")
>                 continue
>             else:
>                 logger.info(f"scheduler request {request.request_id} is ready")
>             # Number of tokens to be scheduled.
>             # We use `request.num_tokens` instead of
>             # `request.num_prompt_tokens` to consider the resumed requests,
>             # which have output tokens.
>             num_new_tokens = request.num_tokens - num_computed_tokens
>             num_new_tokens = num_new_tokens \
>                 if token_budget >= 1 else 0
>             # if num_new_tokens == 0:
>             #     # This happens when prompt length is divisible by the block
>             #     # size and all blocks are cached. Now we force to recompute
>             #     # the last block. Note that we have to re-compute an entire
>             #     # block because allocate_slots() assumes num_computed_tokens
>             #     # is always a multiple of the block size. This limitation
>             #     # can potentially be removed in the future to slightly
>             #     # improve the performance.
>             #     num_computed_tokens -= self.block_size
>             #     num_new_tokens = self.block_size
>             #     computed_blocks.pop()
>             # num_new_tokens = min(num_new_tokens, token_budget)
>             #assert num_new_tokens > 0
> 
>             # yhc: We don't need this in our test
>             # # Schedule encoder inputs.
>             # (encoder_inputs_to_schedule, num_new_tokens,
>             #     new_encoder_budget) = self._try_schedule_encoder_inputs(
>             #         request, num_computed_tokens, num_new_tokens,
>             #         encoder_budget)
>             if num_new_tokens == 0:
>                 # The request cannot be scheduled.
>                 logger.debug(f"scheduler checker request {request.request_id} is put into remaining_requests num_new_tokens == 0")
>                 remaining_requests.append(request)
>                 continue
> 
>             new_blocks = self.kv_cache_manager.allocate_slots(
>                 request, num_new_tokens, computed_blocks)
>     
>             if new_blocks is None:
>                 # The request cannot be scheduled.
>                 logger.debug(f"scheduler checker request {request.request_id} is put into remaining_requests new_blocks is None")
>                 remaining_requests.append(request)
>                 continue
>             if request.num_tokens == request.num_prompt_tokens:
>                 prefill_requests.append(request)
>                 prefill_scheduled_tokens[request.request_id] = num_new_tokens
>             if request.status == RequestStatus.WAITING:
>                 # We do these in schedule()
>                 # scheduled_new_reqs.append(request)
>                 pass
>             else:
>                 raise RuntimeError(
>                     f"Invalid status for kv_recv request: {request.status}")
> 
>             req_to_new_block_ids[request.request_id] = [
>                 b.block_id for b in computed_blocks + new_blocks
>             ]
>             token_budget -= 1
>             request.status = RequestStatus.RUNNING
>             request.num_computed_tokens = num_computed_tokens
> 
>             # yhc: Ignore
>             # # Encoder-related.
>             # if encoder_inputs_to_schedule:
>             #     scheduled_encoder_inputs[request.request_id] = (
>             #         encoder_inputs_to_schedule)
>             #     # Allocate the encoder cache.
>             #     for i in encoder_inputs_to_schedule:
>             #         self.encoder_cache_manager.allocate(request, i)
>             #     encoder_budget = new_encoder_budget
>         self.waiting = remaining_requests
>         # If we need to recv kv, pause all running requests。
>         # This means we retain their KV blocks and metadata, 
>         # but temporarily halt their execution.
>         # running -> paused
>         if len(prefill_requests) > 0:
>             self.paused.extend(self.running)
>             self.running = prefill_requests
>         return token_budget, prefill_requests, prefill_scheduled_tokens
> 
520c704
< 
---
>         logger.info(f"finish_requests {request_ids}")
534a719,722
>         logger.info(f"_free_request requests req_id {request.request_id}")
>         if has_kv_transfer_group():
>             trigger_send = request.status == RequestStatus.FINISHED_LENGTH_CAPPED
>             get_kv_transfer_group().finish_request(request.request_id, trigger_send)
631a820
>     freeze_req_ids: Set[str]
Binary files v1/engine/__pycache__/__init__.cpython-312.pyc and /workspace/vllm/vllm/v1/engine/__pycache__/__init__.cpython-312.pyc differ
Binary files v1/engine/__pycache__/async_llm.cpython-312.pyc and /workspace/vllm/vllm/v1/engine/__pycache__/async_llm.cpython-312.pyc differ
Binary files v1/engine/__pycache__/core.cpython-312.pyc and /workspace/vllm/vllm/v1/engine/__pycache__/core.cpython-312.pyc differ
Binary files v1/engine/__pycache__/core_client.cpython-312.pyc and /workspace/vllm/vllm/v1/engine/__pycache__/core_client.cpython-312.pyc differ
Binary files v1/engine/__pycache__/detokenizer.cpython-312.pyc and /workspace/vllm/vllm/v1/engine/__pycache__/detokenizer.cpython-312.pyc differ
Binary files v1/engine/__pycache__/mm_input_mapper.cpython-312.pyc and /workspace/vllm/vllm/v1/engine/__pycache__/mm_input_mapper.cpython-312.pyc differ
Binary files v1/engine/__pycache__/output_processor.cpython-312.pyc and /workspace/vllm/vllm/v1/engine/__pycache__/output_processor.cpython-312.pyc differ
Binary files v1/engine/__pycache__/processor.cpython-312.pyc and /workspace/vllm/vllm/v1/engine/__pycache__/processor.cpython-312.pyc differ
diff -r v1/engine/async_llm.py /workspace/vllm/vllm/v1/engine/async_llm.py
237c237,238
<         except asyncio.CancelledError:
---
>         except asyncio.CancelledError as e:
>             logger.exception(f"AsyncLLM EngineClient cancelled {str(e)}")
272c273,275
< 
---
>                     if len(processed_outputs.reqs_to_abort) > 0:
>                         logger.warn(f"_run_output_handler abort request {\
>                             processed_outputs.reqs_to_abort}")
diff -r v1/engine/core.py /workspace/vllm/vllm/v1/engine/core.py
64a65
>             kv_transfer_config=vllm_config.kv_transfer_config,
127,128c128,133
< 
<         scheduler_output = self.scheduler.schedule()
---
>         while True:
>             scheduler_output = self.scheduler.schedule()
>             if scheduler_output.total_num_scheduled_tokens > 0:
>                 break
>             logger.warning("schedule 0 token, wait")
>             time.sleep(0.001)
Binary files v1/executor/__pycache__/__init__.cpython-312.pyc and /workspace/vllm/vllm/v1/executor/__pycache__/__init__.cpython-312.pyc differ
Binary files v1/executor/__pycache__/abstract.cpython-312.pyc and /workspace/vllm/vllm/v1/executor/__pycache__/abstract.cpython-312.pyc differ
Binary files v1/metrics/__pycache__/__init__.cpython-312.pyc and /workspace/vllm/vllm/v1/metrics/__pycache__/__init__.cpython-312.pyc differ
Binary files v1/metrics/__pycache__/loggers.cpython-312.pyc and /workspace/vllm/vllm/v1/metrics/__pycache__/loggers.cpython-312.pyc differ
Binary files v1/metrics/__pycache__/stats.cpython-312.pyc and /workspace/vllm/vllm/v1/metrics/__pycache__/stats.cpython-312.pyc differ
Binary files v1/sample/__pycache__/__init__.cpython-312.pyc and /workspace/vllm/vllm/v1/sample/__pycache__/__init__.cpython-312.pyc differ
Binary files v1/sample/__pycache__/metadata.cpython-312.pyc and /workspace/vllm/vllm/v1/sample/__pycache__/metadata.cpython-312.pyc differ
Binary files v1/sample/__pycache__/sampler.cpython-312.pyc and /workspace/vllm/vllm/v1/sample/__pycache__/sampler.cpython-312.pyc differ
Binary files v1/sample/ops/__pycache__/__init__.cpython-312.pyc and /workspace/vllm/vllm/v1/sample/ops/__pycache__/__init__.cpython-312.pyc differ
Binary files v1/sample/ops/__pycache__/penalties.cpython-312.pyc and /workspace/vllm/vllm/v1/sample/ops/__pycache__/penalties.cpython-312.pyc differ
Binary files v1/sample/ops/__pycache__/topk_topp_sampler.cpython-312.pyc and /workspace/vllm/vllm/v1/sample/ops/__pycache__/topk_topp_sampler.cpython-312.pyc differ
Binary files v1/worker/__pycache__/__init__.cpython-312.pyc and /workspace/vllm/vllm/v1/worker/__pycache__/__init__.cpython-312.pyc differ
Binary files v1/worker/__pycache__/block_table.cpython-312.pyc and /workspace/vllm/vllm/v1/worker/__pycache__/block_table.cpython-312.pyc differ
Binary files v1/worker/__pycache__/gpu_input_batch.cpython-312.pyc and /workspace/vllm/vllm/v1/worker/__pycache__/gpu_input_batch.cpython-312.pyc differ
Binary files v1/worker/__pycache__/gpu_model_runner.cpython-312.pyc and /workspace/vllm/vllm/v1/worker/__pycache__/gpu_model_runner.cpython-312.pyc differ
Binary files v1/worker/__pycache__/gpu_worker.cpython-312.pyc and /workspace/vllm/vllm/v1/worker/__pycache__/gpu_worker.cpython-312.pyc differ
diff -r v1/worker/gpu_model_runner.py /workspace/vllm/vllm/v1/worker/gpu_model_runner.py
36a37,40
> # We leverage kv_connector in vllm v0 for now.
> from vllm.worker.model_runner import ModelInputForGPUWithSamplingMetadata
> from vllm.distributed import get_kv_transfer_group
> import hashlib
79c83,87
<         self.max_num_tokens = scheduler_config.max_num_batched_tokens
---
>         # [PD-disagg] Expand position tensors to max_num_batched_recv_tokens
>         logger.info(f"max_tokens {scheduler_config.max_num_batched_tokens} {scheduler_config.max_num_batched_recv_tokens}")
>         self.max_num_compute_tokens = scheduler_config.max_num_batched_tokens
>         self.max_num_tokens = max(scheduler_config.max_num_batched_tokens,
>                                   scheduler_config.max_num_batched_recv_tokens)
233a242,249
>         logger.debug(f"GPUModelRunner._update_states scheduler_output.freeze_req_ids {scheduler_output.freeze_req_ids}")
>         logger.debug(f"GPUModelRunner._update_states before removal self.input_batch {self.input_batch.req_id_to_index}")
> 
>         for req_id in scheduler_output.freeze_req_ids:
>             req_index = self.input_batch.remove_request(req_id)
>             if req_index is not None:
>                 removed_req_indices.append(req_index)
>         logger.debug(f"GPUModelRunner._update_states after removal self.input_batch {self.input_batch.req_id_to_index}")
360a377
>         logger.info(f"number scheduled requests: {self.input_batch.num_reqs}")
364a382,383
>         logger.info("_prepare_inputs total_num_scheduled_tokens" \
>             f" {total_num_scheduled_tokens}")
384a404
>         # logger.info(f"_prepare_inputs num_scheduled_tokens {num_scheduled_tokens}")
427a448,451
> 
>         # yhc: slot mapping: token -> kv_idx
>         # kv_idx = (block_num * block_size) + offset
>         # (block_num is more like block_id)
445d468
< 
734a758,763
> 
>     def tensor_hash(self, tensor: torch.Tensor) -> str:
>         tensor_cpu = tensor.detach().cpu().to(torch.float32)
>         tensor_bytes = tensor_cpu.numpy().tobytes()
>         return hashlib.md5(tensor_bytes).hexdigest()
> 
784a814,844
> 
>         bypass_model_exec = False
>         kv_transfer_time = 0.0
>         hidden_states = None
>         if self.need_recv_kv(self.kv_caches):
>             logger.debug("ModelRunner.execute_model need_recv_kv")
>             start_time = time.time()
>             num_reqs = self.input_batch.num_reqs
>             index_to_id = []
>             for i, req_id in enumerate(self.input_batch.req_ids[:num_reqs]):
>                 assert req_id is not None
>                 index_to_id.append((i, req_id))
>             logger.debug(f"need_recv_kv request index_to_id {index_to_id}")
>             hidden_states, bypass_model_exec = \
>                 get_kv_transfer_group().recv_kv_caches_and_hidden_states_v1(
>                     # model is used to know which layer the current worker
>                     # is working on, so that we can receive KV for only those
>                     # layers.
>                     self.model,
>                     input_ids,
>                     attn_metadata.seq_lens,
>                     attn_metadata.slot_mapping,
>                     kv_caches=self.kv_caches
>                 )
>             end_time = time.time()
>             kv_transfer_time = end_time - start_time
>             logger.info(f"ModelRunner.execute_model need_recv_kv finish, bypass_model_exec {bypass_model_exec}")
> 
>             #logger.debug(f"ModelRunner.execute_model need_recv_kv finish, hidden_states.shape {hidden_states.shape}, hash {self.tensor_hash(hidden_states)}")
> 
> 
787,796c847,893
<         with set_forward_context(attn_metadata, self.vllm_config):
<             positions = self.mrope_positions[:, :num_input_tokens] \
<                 if self.model_config.uses_mrope \
<                 else self.positions[:num_input_tokens]
<             hidden_states = self.model(
<                 input_ids=input_ids,
<                 positions=positions,
<                 kv_caches=self.kv_caches,
<                 attn_metadata=None,
<                 inputs_embeds=inputs_embeds,
---
>         if not bypass_model_exec:
>             with set_forward_context(attn_metadata, self.vllm_config):
>                 positions = self.mrope_positions[:, :num_input_tokens] \
>                     if self.model_config.uses_mrope \
>                     else self.positions[:num_input_tokens]
> 
>                 logger.debug(f"GPUModelRunner.execute_model input_ids {input_ids}")
>                 hidden_states = self.model(
>                     input_ids=input_ids,
>                     positions=positions,
>                     kv_caches=self.kv_caches,
>                     attn_metadata=None,
>                     inputs_embeds=inputs_embeds,
>                 )
> 
>         if self.need_send_kv(self.kv_caches):
>             logger.debug("ModelRunner.execute_model need_send_kv")
> 
>             num_reqs = self.input_batch.num_reqs
>             is_full_prefill = [False] * num_reqs
>             
>             logger.debug(f"ModelRunner.execute_model need_send_kv num_reqs {num_reqs}")
>             computed = []
>             scheduled_lens = []
>             for i, req_id in enumerate(self.input_batch.req_ids[:num_reqs]):
>                 assert req_id is not None
>                 req_state = self.requests[req_id]
>                 computed.append(req_state.num_computed_tokens)
>                 scheduled_lens.append(scheduler_output.num_scheduled_tokens[req_id])
>                 seq_len = (req_state.num_computed_tokens +
>                         scheduler_output.num_scheduled_tokens[req_id])
>                 assert seq_len <= req_state.num_tokens
>                 # set is_full_prefill to False, never trigger send task here
>                 # if seq_len == req_state.num_tokens:
>                 #     is_full_prefill[i] = True
>             get_kv_transfer_group().send_kv_caches_and_hidden_states_v1(
>                 # model_executable is used to know which layer the current
>                 # worker is working on, so that we can send KV for only those
>                 # layers.
>                 self.model,
>                 input_ids,
>                 self.input_batch.req_ids,
>                 is_full_prefill,
>                 scheduled_lens,
>                 attn_metadata.slot_mapping,
>                 self.kv_caches,
>                 hidden_states[:num_scheduled_tokens],
798a896
>         
801d898
< 
808d904
< 
817a914,917
>             logger.debug(f"GPUModelRunner.execute_model req_id {req_id}")
>             logger.debug(f"GPUModelRunner.execute_model req_state {req_state}")
>             logger.debug(f"GPUModelRunner.execute_model seq_len {seq_len}")
>             logger.debug(f"GPUModelRunner.execute_model req_state.num_tokens {req_state.num_tokens}")
843a944
>             # logger.info(f"ModelRunner.execute_model for loop {i} {req_state.req_id} {req_state.num_computed_tokens} {seq_len}")
874a976,1020
>     def need_recv_kv(self, kv_caches) -> bool:
>         """Check if we need to receive kv-cache from the other worker.
>         We need to receive KV when
>             1. current vLLM instance is KV cache consumer/decode vLLM instance
>             2. this batch is not a profiling run
>             3. this batch is a prefill run
>             
>         Args:
>             model_input: input to the model executable
>             kv_caches: vLLM's paged memory
>         """
> 
>         if self.vllm_config.kv_transfer_config is None or\
>             self.vllm_config.kv_transfer_config.is_kv_producer:
>             return False
>         # check if the current run is profiling
>         is_profile_run = (kv_caches[0].numel() == 0)
>         # check if the current run is prefill
>         is_prefill_run = (self.input_batch.num_prompt_tokens[0] == self.input_batch.num_tokens[0])
> 
>         return self.vllm_config.kv_transfer_config.is_kv_consumer and (
>             not is_profile_run) and is_prefill_run
> 
>     def need_send_kv(self, kv_caches) -> bool:
>         """Check if we need to send kv-cache to the other worker.
>         We need to send KV when
>             1. current vLLM instance is KV cache producer/prefill vLLM instance
>             2. this batch is not a profiling run
>             3. this batch is a prefill run
>             
>         Args:
>             model_input: input to the model executable
>             kv_caches: vLLM's paged memory
>         """
> 
>         if self.vllm_config.kv_transfer_config is None or\
>             self.vllm_config.kv_transfer_config.is_kv_consumer:
>             return False
> 
>         # check if the current run is profiling
>         is_profile_run = (kv_caches[0].numel() == 0)
>         # we will check send condition at request level
>         return self.vllm_config.kv_transfer_config.is_kv_producer and (
>             not is_profile_run)
> 
959c1105
<                 seq_len=self.max_num_tokens,
---
>                 seq_len=self.max_num_compute_tokens,
1009c1155
<         hidden_states = self._dummy_run(self.max_num_tokens, dummy_kv_caches)
---
>         hidden_states = self._dummy_run(self.max_num_compute_tokens, dummy_kv_caches)
1011c1157
<         logits = logits[:self.max_num_tokens]
---
>         logits = logits[:self.max_num_compute_tokens]
1113c1259
<         return kv_cache_spec
---
>         return kv_cache_spec
\ No newline at end of file
diff -r v1/worker/gpu_worker.py /workspace/vllm/vllm/v1/worker/gpu_worker.py
14c14,15
< from vllm.distributed import (ensure_model_parallel_initialized,
---
> from vllm.distributed import (ensure_kv_transfer_initialized,
>                               ensure_model_parallel_initialized,
123c124
<         init_worker_distributed_environment(self.parallel_config, self.rank,
---
>         init_worker_distributed_environment(self.vllm_config, self.rank,
253c254
<     parallel_config: ParallelConfig,
---
>     vllm_config: VllmConfig,
258a260
>     parallel_config = vllm_config.parallel_config
266c268
< 
---
>     ensure_kv_transfer_initialized(vllm_config)
